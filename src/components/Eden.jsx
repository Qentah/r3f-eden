/*
Auto-generated by: https://github.com/pmndrs/gltfjsx
*/

import React, { useEffect, useRef, useState } from "react";
import { useFrame } from "@react-three/fiber";
import { useGLTF, useAnimations } from "@react-three/drei";
import { button, useControls } from "leva";

import * as THREE from "three";
import { useChat } from "../hooks/useChat";

//TODO: Blink eyes
//TODO: Smile

//TTS Config
import * as SpeechSDK from "microsoft-cognitiveservices-speech-sdk";
const subKey = "37e0ba695f324c639507bf0cfdb55539";
const region = "westeurope";
const speechConfig = SpeechSDK.SpeechConfig.fromSubscription(subKey, region);
speechConfig.speechSynthesisVoiceName = "fr-FR-BrigitteNeural";
speechConfig.speechSynthesisLanguage = "fr-FR";
const azureIDtoViseme = {
  0: "viseme_sil",
  1: "viseme_aa",
  2: "viseme_aa",
  3: "viseme_O",
  4: "viseme_E",
  5: "viseme_E",
  6: "viseme_I",
  7: "viseme_U",
  8: "viseme_O",
  9: "viseme_O",
  10: "viseme_U",
  11: "viseme_E",
  12: "viseme_RR",
  13: "viseme_RR",
  14: "viseme_nn",
  15: "viseme_SS",
  16: "viseme_CH",
  17: "viseme_DD",
  18: "viseme_FF",
  19: "viseme_DD",
  20: "viseme_kk",
  21: "viseme_PP",
}
const ssml = (text) => `
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xmlns:mstts="http://www.w3.org/2001/mstts" xml:lang="fr-FR">
<voice name="fr-FR-BrigitteNeural">
    <mstts:viseme type="FacialExpression"/>
    ${text}
</voice>
</speak>`;
const destination = new SpeechSDK.SpeakerAudioDestination();
destination.onAudioStart = (_) => {
  destination.mute();
};
const audioConfig = SpeechSDK.AudioConfig.fromSpeakerOutput(destination);
function playTTS(text) {
  const ssmlText = ssml(text);
  const synthesizer = new SpeechSDK.SpeechSynthesizer(speechConfig, audioConfig);
  const visemeQueue = [];
  synthesizer.visemeReceived = async function (synth, event) {
    if (event.audioOffset != 0)
      visemeQueue.push(event);
  };
  return new Promise((resolve, reject) => {
    synthesizer.speakSsmlAsync(
      ssmlText,
      (result) => {
        const audio = new Audio("data:audio/mp3;base64," + btoa(String.fromCharCode(...new Uint8Array(result.audioData))));
        resolve({ audio, visemeQueue });
      },
      (error) => {
        reject(error);
      }
    );
  });
}
//END TTS Config

export function Eden(props) {
  const { nodes, materials, scene } = useGLTF("/models/avatar.glb");
  const { animations } = useGLTF("/models/animations.glb");
  const group = useRef();
  const { actions, mixer } = useAnimations(animations, group);
  const [animation, setAnimation] = useState(
    animations.find((a) => a.name === "Idle") ? "Idle" : animations[0].name // Check if Idle animation exists otherwise use first animation
  );

  const { fakeMessage, setCameraZoomed } = useChat();
  const [visemes, setVisemes] = useState();
  const [audio, setAudio] = useState();

  useEffect(() => {
    if (fakeMessage) {
      playTTS(fakeMessage).then(({ audio, visemeQueue }) => {
        setAudio(audio);
        setVisemes(visemeQueue);
      });
    }
  }, [fakeMessage]);

  useEffect(() => {
    if (audio) {
      audio.play();
      setCameraZoomed(true);
      setAnimation("Talking_1");
      audio.onended = () => {
        setCameraZoomed(false);
        setAnimation("Idle");
        setAudio();
      };
    }
    return () => {
      if (audio) {
        audio.pause();
        setCameraZoomed(false);
        setAnimation("Idle");
      }
    }
  }, [audio]);

  useFrame(() => {
    Object.keys(nodes.Wolf3D_Avatar.morphTargetDictionary).forEach((key) => {
      if (key === "eyeBlinkLeft" || key === "eyeBlinkRight") {
        return; // eyes wink/blink are handled separately
      }
      lerpMorphTarget(key, 0, 0.2);
    });

    if (audio) {
      const time = audio.currentTime;
      for (let i = 1; i < visemes.length; i++) {
        const prev = visemes[i - 1].audioOffset / 10000000;
        const cur = visemes[i].audioOffset / 10000000;
        if (time >= prev && time < cur) {
          lerpMorphTarget(azureIDtoViseme[visemes[i].visemeId], 1, 0.2);
        }
      }
    }
  });

  useEffect(() => {
    try {
      actions[animation]
        .reset()
        .fadeIn(mixer.stats.actions.inUse === 0 ? 0 : 0.5)
        .play();
    } catch (error) {//TODO: Fix this
    }
    return () => actions[animation].fadeOut(0.5);
  }, [animation]);

  const lerpMorphTarget = (target, value, speed = 0.1) => {
    scene.traverse((child) => {
      if (child.isSkinnedMesh && child.morphTargetDictionary) {
        const index = child.morphTargetDictionary[target];
        if (
          index === undefined ||
          child.morphTargetInfluences[index] === undefined
        ) {
          return;
        }
        child.morphTargetInfluences[index] = THREE.MathUtils.lerp(
          child.morphTargetInfluences[index],
          value,
          speed
        );

        // try {
        //   set({
        //     [target]: value,
        //   });
        // } catch (e) { }
      }
    });
  };


  // CONTROLS

  useControls("Animations", {
    animation: {
      value: animation,
      options: animations.map((a) => a.name),
      onChange: (value) => setAnimation(value),
    },
  });

  const [, set] = useControls("MorphTarget", () =>
    Object.assign(
      {},
      ...Object.keys(nodes.Wolf3D_Avatar.morphTargetDictionary).map((key) => {
        return {
          [key]: {
            label: key,
            value: 0,
            min: 0,
            max: 1,
            onChange: (val) => {
              lerpMorphTarget(key, val, 1);
            },
          },
        };
      })
    )
  );

  return (
    <group {...props} ref={group} >
      <primitive object={nodes.Hips} />
      <skinnedMesh
        name="Wolf3D_Avatar"
        geometry={nodes.Wolf3D_Avatar.geometry}
        material={materials.Wolf3D_Avatar}
        skeleton={nodes.Wolf3D_Avatar.skeleton}
        morphTargetDictionary={nodes.Wolf3D_Avatar.morphTargetDictionary}
        morphTargetInfluences={nodes.Wolf3D_Avatar.morphTargetInfluences}
        castShadow
      />
    </group>
  );
}
useGLTF.preload('/models/avatar.glb')
